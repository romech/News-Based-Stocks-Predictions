path:
  # source
  news: ../data/source/RedditNews.tsv
  stocks: ../data/source/DJIA_table.csv
  combined: ../data/source/Combined_News_DJIA.csv
  # intermediate
  tokenized: ../data/intermediate/news_tokenized.csv
  stemmed: ../data/intermediate/news_stemmed.csv
  stems-split: ../data/intermediate/stems_{}.txt # meaningful for train only
  split-tags: ../data/intermediate/stems_{}_dates.txt
  splits-tags: ../data/intermediate/splits.json

  # LDA
  lda-log: ../data/intermediate/lda/{}/lda.log
  lda-summary: ../data/intermediate/lda/{}/summary.tsv
  lda-model: ../data/intermediate/lda/{}/model
  lda-vis: ../data/intermediate/lda/{}/visualization.html
  lda-pointer: ../data/intermediate/lda/latest.txt
  # Further training
  topic-distr: ../data/intermediate/topics_{}.txt # probably useless
  context-topics: ../data/prepared/topics_{}.csv
  target: ../data/prepared/target_{}.csv

# Tokenizig (see https://nlp.stanford.edu/software/tokenizer.html)
stanford-tokenizer:
  classpath: '../stanford-corenlp-full-2018-10-05/*'
  parameters:
    - preserveLines
    - lowerCase
  exclude:
    - '[a-zA-Z]' # single letter
    - '[^\w]+'   # sequence of non-digits, non-letters
    - "'.+"      # starting with '
    - '\d{1,3}'  # meaningless numbers
  options:
    - "ptb3Escaping=false"
    - "normalizeAmpersandEntity=true"
    - "splitHyphenated=true"

# текст пропускается через: токенизатор, стеммер/лемматизатор, фильтр по длине и stopwords
text-simplifying:
  min-length: 2
  stemming: false
  lemmatizing: true

# разделение дней на train-test по интервалам. В процентах от начала
splits:
  train: [0, 50]
  test: [50, 85]
  holdout: [85, 101]

lda:
  word-extremes:
    min-count: 3
    max-freq: 0.02

  topics: 100
  epochs: 100
  chunk-size: 5000
#  eval-every: 100

  visualization: true

regression-data:
  num-days-context: 3
