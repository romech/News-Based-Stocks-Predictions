path:
  # source
  news: data/source/RedditNews.tsv
  stocks: data/source/DJIA_table.csv
  combined: data/source/Combined_News_DJIA.csv
  fbprophet: data/source/FbProphecies.csv
  fasttext-train: data/source/another_news_corpus.txt
  # tokenizing, stemming, splitting
  tokenized: data/intermediate/news_tokenized.csv
  stemmed: data/intermediate/news_stemmed.csv
  stems-split: data/intermediate/stems_{}.txt # meaningful for train only
  split-tags: data/intermediate/stems_{}_dates.txt
  splits-tags: data/intermediate/splits.json
  # sentiment analysis
  sentiments: data/prepared/sentiments.csv
  article-polarity: data/prepared/article_sentiments.tsv
  # WordEmbeddings
  prepare-fasttext: data/intermediate/prep_train_fast.txt
  fasttext-model: data/intermediate/fasttext/model.bin
  article-embeds: data/prepared/article_vectors.txt
  # LDA
  lda-log: data/intermediate/lda/{}/lda.log
  lda-summary: data/intermediate/lda/{}/summary.tsv
  lda-model: data/intermediate/lda/{}/model
  lda-vis: data/intermediate/lda/{}/visualization.html
  lda-pointer: data/intermediate/lda/latest.txt
  # Further training
  topic-distr: data/intermediate/topics_{}.txt # probably useless
  day-features: data/prepared/features_{}.csv
  features-descr: data/prepared/features_description.json
  target: data/prepared/target_{}.csv

# Tokenizig (see https://nlp.stanford.edu/software/tokenizer.html)
stanford-tokenizer:
  classpath: 'stanford-corenlp-full-2018-10-05/*'
  parameters:
    - preserveLines
    - lowerCase
  exclude:
    - '[a-zA-Z]' # single letter
    - '[^\w]+'   # sequence of non-digits, non-letters
    - "'.+"      # starting with '
    - '\d{1,3}'  # meaningless numbers
  options:
    - "ptb3Escaping=false"
    - "normalizeAmpersandEntity=true"
    - "splitHyphenated=true"

# текст пропускается через: токенизатор, стеммер/лемматизатор, фильтр по длине и stopwords
text-simplifying:
  min-length: 2
  stemming: true
  lemmatizing: false

# разделение дней на train-test по интервалам. В процентах от начала
splits:
  train: [0, 65]
  test: [65, 90]
  holdout: [90, 101]

lda:
  word-extremes:
    min-count: 3
    max-freq: 0.02

  topics: 300
  epochs: 1500
  chunk-size: 6000
#  eval-every: 100

  visualization: true

regression-data:
  num-days-context: 1
  sentiment-percentiles: [0, 33, 66, 100]

random-forest:
  n-estimators: 300
  update-every: 20
  add-estimators: 2
